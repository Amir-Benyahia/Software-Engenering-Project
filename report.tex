\documentclass[11pt,a4paper,twoside]{article}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[a4paper,margin=1.25in,headheight=13.6pt]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[svgnames]{xcolor}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=SteelBlue,
  urlcolor=SteelBlue,
  citecolor=SeaGreen
}

% --- METADATA MACROS (EASY TO EDIT) ---
\newcommand{\university}{Université Côte d'Azur}
\newcommand{\program}{Master 1 Informatique}
\newcommand{\course}{Génie Logiciel (Software Engineering)}
\newcommand{\instructor}{Jean-Charles Régin}
\newcommand{\student}{Amir Benyahia}
\newcommand{\academicyear}{2024--2025}

% --- HEADERS/FOOTERS ---
\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO,LE]{\small \slshape Software Engineering Project}
\fancyfoot[CE,CO]{\small M1 Informatique}
\fancyfoot[RE,LO]{\small Amir Benyahia}
\fancyfoot[LE,RO]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

% ======================================
\begin{document}
% ======================================

% --- TITLE PAGE ---
\begin{titlepage}
    \centering
    \vspace*{1.5cm}
    % University logo (place a file named 'uca_logo.png' next to this .tex)
    \IfFileExists{uca_logo.png}{%
      \includegraphics[width=0.28\textwidth]{uca_logo.png}\\[1.2cm]
    }{%
      {\Large \textbf{\university}}\\[1.2cm]
    }

    {\Large \textsf{\program}}\\[0.25cm]
    {\normalsize \textsf{\course}}\\[0.75cm]

    {\color{SteelBlue}\rule{\textwidth}{1.2pt}}\\[0.65cm]
    {\huge \bfseries Bit Packing for Integer Arrays}\\[0.25cm]
    {\Large Software Engineering Project}\\[0.65cm]
    {\color{SteelBlue}\rule{\textwidth}{1.2pt}}\\[1.2cm]

    \begin{tabular}{@{}ll@{}}
      	extbf{Student:}    & \student \\
      	extbf{Instructor:} & \instructor \\
      	extbf{Academic year:} & \academicyear \\
      	extbf{University:} & \university \\
    \end{tabular}

    \vfill
    {\large \today}\par
\end{titlepage}

\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}

% --- INTRO ---
\section{Introduction}

\subsection{Context}
Transmitting integer arrays efficiently is a common need. When values are small, using full 32-bit integers wastes bandwidth. This report studies and implements \textit{Bit Packing} to compress arrays while still supporting direct access to the i-th value after compression.

\subsection{Objectives}
We aim to:
\begin{itemize}
  \item compute the minimum number of bits $k$ needed to represent the maximum of the input,
  \item store each integer using exactly $k$ bits,
  \item support direct access via \texttt{get(i)} without full decompression,
  \item implement two versions (non-spanning and spanning), plus an overflow variant,
  \item measure the performance of \texttt{compress}, \texttt{decompress} and \texttt{get}.
\end{itemize}

This report presents the design, implementation and performance evaluation of the system.

% --- DESIGN ---
\section{Software Design and Architecture}

\subsection{Environment and Constraints}
Implementation is in Python 3. To emulate fixed 32-bit blocks, we use \texttt{array('I')} (unsigned 32-bit C integers). Because Python integers are arbitrary precision, we explicitly mask writes with \texttt{0xFFFFFFFF} where needed to avoid unintended overflow.

\subsection{Main Components}
The codebase is organized as follows:
\begin{itemize}
  \item \textbf{Interface}: \texttt{IntegerCompressor} with \texttt{compress}, \texttt{decompress}, \texttt{get},
  \item \textbf{Strategies}:
  \texttt{BitPackingNonSpanning}, \texttt{BitPackingSpanning}, \texttt{BitPackingOverflow},
  \item \textbf{Factory}: \texttt{CompressorFactory} (selects the compressor by name),
  \item \textbf{CLI/Demo}: \texttt{main.py},
  \item \textbf{Benchmark}: \texttt{benchmark.py} (uses \texttt{timeit}).
\end{itemize}

\subsection{Design Patterns}
We use two patterns:
\begin{itemize}
  \item \textbf{Factory}: decouples client code from concrete implementations. Adding a new compressor only touches the factory.
  \item \textbf{Decorator}: \texttt{BitPackingOverflow} wraps a \texttt{BitPackingSpanning} compressor to add overflow handling without duplicating low-level bit packing logic.
\end{itemize}

% --- IMPLEMENTATION ---
\section{Implementation Details}

\subsection{Non-Spanning Bit Packing}
We compute $k=\texttt{max(data).bit\_length()}$ (or 1 for an all-zero array). We then pack $\lfloor 32/k \rfloor$ elements per 32-bit slot. This wastes trailing bits but yields simple $O(1)$ indexing: an integer is at
\[
\text{array\_index} = \left\lfloor \frac{i}{\text{elements\_per\_int}} \right\rfloor,\quad
\text{bit\_offset} = (i \bmod \text{elements\_per\_int}) \times k.
\]

\subsection{Spanning Bit Packing}
We maintain a global \texttt{bit\_cursor} and write each $k$-bit value possibly across two consecutive 32-bit integers. Before OR-ing into the \texttt{array('I')}, we mask with \texttt{0xFFFFFFFF} to keep only the lower 32 bits. The \texttt{get(i)} function reconstructs a $k$-bit value by reading one or two 32-bit cells and masking.

\subsection{Overflow Bit Packing}
We introduce a smaller width $k'$ for the \emph{main} area and reserve the maximum value $(1 \ll k') - 1$ as a sentinel. Values $\ge$ sentinel go to an \emph{overflow area} stored separately. During \texttt{get(i)}, if the main value is the sentinel, we count how many sentinels appear before $i$ to index the overflow array.

\paragraph{Note on complexity.}
The naive \texttt{get(i)} for overflow is $O(n)$ in the worst case (counting prior sentinels). A simple improvement is to precompute a prefix count or a list of overflow positions during \texttt{compress} to obtain $O(1)$ average-time access. This is left as future work.

\paragraph{Main bits default.}
In the CLI (\texttt{main.py}), \texttt{--main-bits} defaults to 3. In the factory, if no argument is given, the default is 8. For benchmarking and examples, we explicitly set $k'=10$ to avoid ambiguity.

% --- BENCHMARK ---
\section{Benchmark and Performance Analysis}

\subsection{Protocol}
The script \texttt{benchmark.py} uses \texttt{timeit} with:
\begin{itemize}
  \item dataset size: $N=10{,}000$ integers,
  \item $k'=10$ for overflow,
  \item roughly $5\%$ overflow values (randomly selected),
  \item functions measured: \texttt{compress} (10 runs), \texttt{decompress} (100 runs), \texttt{get(i)} (1000 runs) with $i=N/2$.
\end{itemize}

\subsection{Results (example run)}
Times depend on hardware. The table below is illustrative; please refer to the script output for your machine.

\begin{table}[hbt!]
\centering
\caption{Example benchmark on a 10,000-integer dataset (average over runs).}
\label{tab:benchmark}
\begin{tabular}{@{}llc@{}}
\toprule
\textbf{Function} & \textbf{Compressor} & \textbf{Average Time (s)} \\ \midrule
\texttt{compress()}    & \texttt{spanning} & 0.0827 \\
\texttt{compress()}    & \texttt{overflow} & 0.0828 \\
\addlinespace
\texttt{decompress()}  & \texttt{spanning} & 0.0053 \\
\texttt{decompress()}  & \texttt{overflow} & 0.0054 \\
\addlinespace
\texttt{get(i)}        & \texttt{spanning} & $< 10^{-6}$ \\
\texttt{get(i)}        & \texttt{overflow} & $< 10^{-6}$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion}
\begin{itemize}
  \item \textbf{Compression}: spanning and overflow are close; splitting into main/overflow adds little cost.
  \item \textbf{Decompression}: similar as well; per-element overhead is small.
  \item \textbf{Access}: \texttt{get(i)} is effectively instantaneous at this scale; however, overflow can degrade if many sentinels occur and we do not precompute indices (future optimization).
\end{itemize}

\subsection{When is compression worthwhile?}
Let $S_{\text{orig}}$ be the uncompressed size (bytes), $S_{\text{comp}}$ the compressed size, $B$ the network throughput (bytes/s), and $T_{\text{comp}}$ the compression time. A simple model for total time with compression is:
\[
T_{\text{with}} = T_{\text{comp}} + \frac{S_{\text{comp}}}{B},\quad
T_{\text{without}} = \frac{S_{\text{orig}}}{B}.
\]
Compression is beneficial if $T_{\text{with}} < T_{\text{without}}$, i.e.
\[
T_{\text{comp}} < \frac{S_{\text{orig}} - S_{\text{comp}}}{B}.
\]
This threshold directly ties the gain to the compression ratio and the network speed. If we include client-side \texttt{decompress()} in the model, just replace $T_{\text{comp}}$ by $T_{\text{comp}} + T_{\text{decomp}}$.

% --- LIMITS/BONUS ---
\section{Limits and Bonus (Negative Integers)}
Current implementation assumes non-negative 32-bit integers. Handling negatives can be added:
\begin{itemize}
  \item \textbf{Sign bit}: store $\lvert x\rvert$ with one extra bit for the sign,
  \item \textbf{ZigZag}: map $\mathbb{Z}\!\to\!\mathbb{N}$ (e.g., $0\mapsto0$, $-1\mapsto1$, $1\mapsto2$...) then compress as unsigned.
\end{itemize}

% --- CONCLUSION ---
\section{Conclusion}
We implemented and compared three bit-packing strategies fulfilling the project brief: \emph{non-spanning}, \emph{spanning} and \emph{overflow}. The architecture (Factory + Decorator) keeps the code modular and easy to extend. In practice:
\begin{itemize}
  \item \textbf{Non-spanning} is the simplest to reason about. It provides very cheap indexing (one division and one shift) but wastes the remaining bits in each 32-bit word, so the compression ratio is weaker when $k$ does not divide 32.
  \item \textbf{Spanning} is a strong default: it achieves near-optimal packing for a fixed $k$ while keeping \texttt{get(i)} in $O(1)$ (read one or two words, then mask/shift). The implementation is slightly more involved but pays off in space.
  \item \textbf{Overflow} is attractive on heavy-tailed data: most values are small and a few are very large. With a good choice of $k'$ (we used $k'=10$ in the benchmark), the main area stays compact and only outliers spill to the overflow area. The current \texttt{get(i)} is correct but can degrade toward $O(n)$ if sentinels are frequent; caching prefix counts or positions fixes this in future work.
\end{itemize}

Our measurements with \texttt{timeit} show that the CPU cost of (de)compression is small compared to typical transmission times when the compression ratio is significant. The threshold model \big($T_{\text{comp}} < \frac{S_{\text{orig}}-S_{\text{comp}}}{B}$, optionally adding $T_{\text{decomp}}$\big) gives a clear decision rule: faster networks or poor compression ratios reduce the benefit, while slower links or better ratios amplify it.

\paragraph{Practical guidance.} Use \textbf{spanning} by default. Prefer \textbf{non-spanning} if implementation simplicity or strict alignment constraints matter more than space. Switch to \textbf{overflow} when a small $k'$ shrinks the main area and only a small fraction of values (e.g., $\le 5\%-10\%$) overflow; pick $k'$ from a quick histogram to keep the sentinel rate low. By design, values $\ge (1\ll k')-1$ are treated as overflow (sentinel semantics is inclusive), which should be documented for users.

\paragraph{Limitations and next steps.} The current code targets non-negative 32-bit integers. Future work includes: (i) adding an index cache for overflow access in $O(1)$, (ii) supporting negative values (e.g., ZigZag), (iii) auto-tuning $k'$ from the observed distribution, and (iv) exposing the break-even calculation directly in the CLI to help users decide when to compress on their link.

% ======================================
\end{document}
% ======================================
